{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Given corpus\n",
        "corpus = [\n",
        "    'the sun is a star',\n",
        "    'the moon is a satellite',\n",
        "    'the sun and moon are celestial bodies'\n",
        "]\n",
        "\n",
        "# Tokenize and build vocabulary\n",
        "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
        "vocab = sorted(set(word for doc in tokenized_corpus for word in doc))\n",
        "\n",
        "# Compute Document Frequency (DF)\n",
        "def compute_df(tokenized_corpus, vocab):\n",
        "    df = {}\n",
        "    for word in vocab:\n",
        "        df[word] = sum(1 for doc in tokenized_corpus if word in doc)\n",
        "    return df\n",
        "\n",
        "# Compute Inverse Document Frequency (IDF)\n",
        "def compute_idf(df, N):\n",
        "    idf = {}\n",
        "    for word, freq in df.items():\n",
        "        idf[word] = math.log(N / freq)\n",
        "    return idf\n",
        "\n",
        "# Compute Term Frequency (TF) for a document\n",
        "def compute_tf(doc, vocab):\n",
        "    tf = {}\n",
        "    counts = Counter(doc)\n",
        "    doc_len = len(doc)\n",
        "    for word in vocab:\n",
        "        tf[word] = counts[word] / doc_len\n",
        "    return tf\n",
        "\n",
        "# Compute TF-IDF for all documents\n",
        "def compute_tfidf(tokenized_corpus, vocab):\n",
        "    N = len(tokenized_corpus)\n",
        "    df = compute_df(tokenized_corpus, vocab)\n",
        "    idf = compute_idf(df, N)\n",
        "    tfidf_matrix = []\n",
        "    for doc in tokenized_corpus:\n",
        "        tf = compute_tf(doc, vocab)\n",
        "        tfidf = [tf[word] * idf[word] for word in vocab]\n",
        "        tfidf_matrix.append(tfidf)\n",
        "    return tfidf_matrix, vocab, idf\n",
        "\n",
        "manual_tfidf_matrix, vocab, manual_idf = compute_tfidf(tokenized_corpus, vocab)\n",
        "\n",
        "print(\"Manual TF-IDF Matrix:\")\n",
        "for row in manual_tfidf_matrix:\n",
        "    print([round(val, 4) for val in row])\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Manual IDF:\", {w: round(v, 4) for w, v in manual_idf.items()})\n",
        "\n",
        "# CountVectorizer and TfidfVectorizer Comparison\n",
        "count_vec = CountVectorizer()\n",
        "count_matrix = count_vec.fit_transform(corpus)\n",
        "print(\"\\nCountVectorizer Matrix:\")\n",
        "print(count_matrix.toarray())\n",
        "print(\"CountVectorizer Vocabulary:\", count_vec.get_feature_names_out())\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(norm=None, use_idf=True, smooth_idf=False)\n",
        "tfidf_matrix = tfidf_vec.fit_transform(corpus)\n",
        "print(\"\\nTfidfVectorizer Matrix (no normalization):\")\n",
        "print(tfidf_matrix.toarray())\n",
        "print(\"TfidfVectorizer Vocabulary:\", tfidf_vec.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Fsj1FEzuuC",
        "outputId": "d6213429-96d1-4dd7-fa90-af16b90c69fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual TF-IDF Matrix:\n",
            "[0.0811, 0.0, 0.0, 0.0, 0.0, 0.0811, 0.0, 0.0, 0.2197, 0.0811, 0.0]\n",
            "[0.0811, 0.0, 0.0, 0.0, 0.0, 0.0811, 0.0811, 0.2197, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.1569, 0.1569, 0.1569, 0.1569, 0.0, 0.0579, 0.0, 0.0, 0.0579, 0.0]\n",
            "Vocabulary: ['a', 'and', 'are', 'bodies', 'celestial', 'is', 'moon', 'satellite', 'star', 'sun', 'the']\n",
            "Manual IDF: {'a': 0.4055, 'and': 1.0986, 'are': 1.0986, 'bodies': 1.0986, 'celestial': 1.0986, 'is': 0.4055, 'moon': 0.4055, 'satellite': 1.0986, 'star': 1.0986, 'sun': 0.4055, 'the': 0.0}\n",
            "\n",
            "CountVectorizer Matrix:\n",
            "[[0 0 0 0 1 0 0 1 1 1]\n",
            " [0 0 0 0 1 1 1 0 0 1]\n",
            " [1 1 1 1 0 1 0 0 1 1]]\n",
            "CountVectorizer Vocabulary: ['and' 'are' 'bodies' 'celestial' 'is' 'moon' 'satellite' 'star' 'sun'\n",
            " 'the']\n",
            "\n",
            "TfidfVectorizer Matrix (no normalization):\n",
            "[[0.         0.         0.         0.         1.40546511 0.\n",
            "  0.         2.09861229 1.40546511 1.        ]\n",
            " [0.         0.         0.         0.         1.40546511 1.40546511\n",
            "  2.09861229 0.         0.         1.        ]\n",
            " [2.09861229 2.09861229 2.09861229 2.09861229 0.         1.40546511\n",
            "  0.         0.         1.40546511 1.        ]]\n",
            "TfidfVectorizer Vocabulary: ['and' 'are' 'bodies' 'celestial' 'is' 'moon' 'satellite' 'star' 'sun'\n",
            " 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj9px0WQxTAs",
        "outputId": "ab6c62c5-0a38-40c9-a0f2-79d619db609f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Add this line to download the missing resource\n",
        "\n",
        "df = pd.read_csv(\"spam.csv\", encoding='latin-1')[['v1', 'v2']]\n",
        "df.columns = ['Label', 'Message']\n",
        "df['Label'] = df['Label'].map({'spam': 1, 'ham': 0})\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "print(\"Loading Word2Vec model (this may take time)...\")\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def vectorize_message(message, model):\n",
        "    words = preprocess(message)\n",
        "    vectors = [model[word] for word in words if word in model]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "X_vectors = np.array([vectorize_message(msg, w2v_model) for msg in df['Message']])\n",
        "y = df['Label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "def predict_message_class(model, w2v_model, message):\n",
        "    vec = vectorize_message(message, w2v_model).reshape(1, -1)\n",
        "    pred = model.predict(vec)[0]\n",
        "    return \"spam\" if pred == 1 else \"ham\"\n",
        "\n",
        "# Example:\n",
        "sample = \"You won a free ticket! Reply now!\"\n",
        "print(\"Predicted Class:\", predict_message_class(clf, w2v_model, sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j84tWMTOw9Ax",
        "outputId": "49924205-36db-48ac-bba7-c2ed647141e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Word2Vec model (this may take time)...\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Test Accuracy: 0.9417040358744395\n",
            "Predicted Class: spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import gensim.downloader\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data (Colab may already have it)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# --- 1. Load the Twitter US Airline Sentiment dataset ---\n",
        "df = pd.read_csv('Tweets.csv')\n",
        "print(df['airline_sentiment'].value_counts())\n",
        "\n",
        "# --- 2. Preprocess each tweet ---\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_tweet(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words and len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "df['processed_tokens'] = df['text'].apply(preprocess_tweet)\n",
        "df = df[df['processed_tokens'].map(len) > 0].reset_index(drop=True)\n",
        "\n",
        "# --- 3. Download and load the pre-trained Google News Word2Vec model ---\n",
        "print(\"Downloading Google News Word2Vec model... This may take a few minutes.\")\n",
        "w2v_model = gensim.downloader.load('word2vec-google-news-300')\n",
        "\n",
        "# --- 4. Convert each tweet to a fixed-length vector (average of word vectors) ---\n",
        "def tweet_to_vector(tokens, w2v_model, vector_size=300):\n",
        "    vectors = [w2v_model[word] for word in tokens if word in w2v_model]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "df['vector'] = df['processed_tokens'].apply(lambda tokens: tweet_to_vector(tokens, w2v_model))\n",
        "\n",
        "# --- 5. Prepare data for machine learning ---\n",
        "X = np.vstack(df['vector'].values)\n",
        "y = df['airline_sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2}).values\n",
        "\n",
        "# --- 6. Split dataset into training (80%) and testing (20%) sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --- 7. Train Multiclass Logistic Regression classifier ---\n",
        "print(\"Training model...\")\n",
        "clf = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# --- 8. Make predictions and evaluate ---\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n",
        "\n",
        "# --- 9. Prediction function ---\n",
        "def predict_tweet_sentiment(model, w2v_model, tweet):\n",
        "    processed_tokens = preprocess_tweet(tweet)\n",
        "    tweet_vector = tweet_to_vector(processed_tokens, w2v_model).reshape(1, -1)\n",
        "    prediction = model.predict(tweet_vector)[0]\n",
        "    sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "    return sentiment_map[prediction]\n",
        "\n",
        "# --- 10. Example predictions ---\n",
        "test_tweets = [\n",
        "    \"@airline Your service was amazing! Best flight ever! 😊\",\n",
        "    \"@airline Delayed 3 hours with no explanation. Terrible experience.\",\n",
        "    \"@airline Flight was okay, nothing special to report.\",\n",
        "    \"Thank you @airline for the upgrade! Great crew and smooth flight!\",\n",
        "    \"@airline Why is customer service so bad? Frustrated passenger here.\",\n",
        "    \"Just boarded @airline flight. Let's see how this goes.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING PREDICTION FUNCTION\")\n",
        "print(\"=\"*60)\n",
        "for tweet in test_tweets:\n",
        "    print(f\"Tweet: {tweet}\")\n",
        "    print(f\"Predicted Sentiment: {predict_tweet_sentiment(clf, w2v_model, tweet)}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvn3FvEo5AbE",
        "outputId": "fc961e47-aaa4-493a-acc1-564e6cd799eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "airline_sentiment\n",
            "negative    9178\n",
            "neutral     3099\n",
            "positive    2363\n",
            "Name: count, dtype: int64\n",
            "Downloading Google News Word2Vec model... This may take a few minutes.\n",
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.7560\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.91      0.85      1835\n",
            "     neutral       0.57      0.40      0.47       615\n",
            "    positive       0.72      0.60      0.66       472\n",
            "\n",
            "    accuracy                           0.76      2922\n",
            "   macro avg       0.70      0.64      0.66      2922\n",
            "weighted avg       0.74      0.76      0.74      2922\n",
            "\n",
            "\n",
            "============================================================\n",
            "TESTING PREDICTION FUNCTION\n",
            "============================================================\n",
            "Tweet: @airline Your service was amazing! Best flight ever! 😊\n",
            "Predicted Sentiment: positive\n",
            "------------------------------------------------------------\n",
            "Tweet: @airline Delayed 3 hours with no explanation. Terrible experience.\n",
            "Predicted Sentiment: negative\n",
            "------------------------------------------------------------\n",
            "Tweet: @airline Flight was okay, nothing special to report.\n",
            "Predicted Sentiment: negative\n",
            "------------------------------------------------------------\n",
            "Tweet: Thank you @airline for the upgrade! Great crew and smooth flight!\n",
            "Predicted Sentiment: positive\n",
            "------------------------------------------------------------\n",
            "Tweet: @airline Why is customer service so bad? Frustrated passenger here.\n",
            "Predicted Sentiment: negative\n",
            "------------------------------------------------------------\n",
            "Tweet: Just boarded @airline flight. Let's see how this goes.\n",
            "Predicted Sentiment: negative\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}